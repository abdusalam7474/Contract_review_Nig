{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np\nimport os\nimport cv2\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom keras import layers\nfrom keras.models import Sequential\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\nfrom keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.layers import Input, Lambda ,Dense ,Flatten ,Dropout\nfrom keras.models import Model\nimport keras\n# define path to dataset\npath = \"/kaggle/input/sign-language-dataset/ImagePro\"\npath2 = \"/kaggle/input/myimagepro/myImagePro\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-07T19:31:37.052609Z","iopub.execute_input":"2023-09-07T19:31:37.053365Z","iopub.status.idle":"2023-09-07T19:31:46.945147Z","shell.execute_reply.started":"2023-09-07T19:31:37.053311Z","shell.execute_reply":"2023-09-07T19:31:46.944099Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preparations","metadata":{}},{"cell_type":"code","source":"#pip install opencv-python","metadata":{"execution":{"iopub.status.busy":"2023-08-31T01:57:08.022695Z","iopub.execute_input":"2023-08-31T01:57:08.023278Z","iopub.status.idle":"2023-08-31T01:57:08.032228Z","shell.execute_reply.started":"2023-08-31T01:57:08.023243Z","shell.execute_reply":"2023-08-31T01:57:08.031033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install cvzone","metadata":{"execution":{"iopub.status.busy":"2023-09-07T19:31:46.949763Z","iopub.execute_input":"2023-09-07T19:31:46.950511Z","iopub.status.idle":"2023-09-07T19:32:01.705378Z","shell.execute_reply.started":"2023-09-07T19:31:46.950473Z","shell.execute_reply":"2023-09-07T19:32:01.704009Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting cvzone\n  Downloading cvzone-1.6.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from cvzone) (4.8.0.74)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from cvzone) (1.23.5)\nBuilding wheels for collected packages: cvzone\n  Building wheel for cvzone (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for cvzone: filename=cvzone-1.6-py3-none-any.whl size=26342 sha256=e956942599754e9085c08277113eb32ccd32005a5e3613df8c0d486866404ffe\n  Stored in directory: /root/.cache/pip/wheels/1e/c1/41/09fa72fe85489b6cf8a6e93ea78c0e98ba80b010466ee23fca\nSuccessfully built cvzone\nInstalling collected packages: cvzone\nSuccessfully installed cvzone-1.6\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install mediapipe","metadata":{"execution":{"iopub.status.busy":"2023-09-07T19:32:01.707787Z","iopub.execute_input":"2023-09-07T19:32:01.708192Z","iopub.status.idle":"2023-09-07T19:32:14.845571Z","shell.execute_reply.started":"2023-09-07T19:32:01.708153Z","shell.execute_reply":"2023-09-07T19:32:14.843776Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting mediapipe\n  Downloading mediapipe-0.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.4.0)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (23.1.0)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (23.5.26)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (3.7.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.23.5)\nRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.10/site-packages (from mediapipe) (4.8.0.74)\nRequirement already satisfied: protobuf<4,>=3.11 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (3.20.3)\nCollecting sounddevice>=0.4.4 (from mediapipe)\n  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\nRequirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (2.8.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\nInstalling collected packages: sounddevice, mediapipe\nSuccessfully installed mediapipe-0.10.3 sounddevice-0.4.6\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-09-07T19:32:14.848531Z","iopub.execute_input":"2023-09-07T19:32:14.848927Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.65.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"#https://drive.google.com/file/d/1e8bDeSfhrS1cRpEt8LO13GFrTlKEHrN7/view?usp=drivesdk\n!gdown --id 1e8bDeSfhrS1cRpEt8LO13GFrTlKEHrN7","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://drive.google.com/file/d/1Jn4cZWXOwoVBXeCizxmI3H6xYhrQyIsP/view?usp=drivesdk\n!gdown --id 1Jn4cZWXOwoVBXeCizxmI3H6xYhrQyIsP","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download the 1st person dataset","metadata":{}},{"cell_type":"code","source":"!unzip \"/kaggle/working/Copy of Dataset.zip\" -d \"/kaggle/working/ImagePro\"\n!unzip \"/kaggle/working/ImagePro.zip\" -d \"/kaggle/working/ImagePro2\"","metadata":{"execution":{"iopub.status.busy":"2023-09-05T01:33:33.603149Z","iopub.execute_input":"2023-09-05T01:33:33.604330Z","iopub.status.idle":"2023-09-05T01:33:37.328979Z","shell.execute_reply.started":"2023-09-05T01:33:33.604284Z","shell.execute_reply":"2023-09-05T01:33:37.327814Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cvzone.HandTrackingModule import HandDetector\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:02:27.487433Z","iopub.execute_input":"2023-09-05T02:02:27.487811Z","iopub.status.idle":"2023-09-05T02:02:27.760076Z","shell.execute_reply.started":"2023-09-05T02:02:27.487782Z","shell.execute_reply":"2023-09-05T02:02:27.758958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detector = HandDetector(detectionCon=0.8, maxHands=2)\nimg = cv2.imread(\"/kaggle/working/ImagePro2/7/2023-09-02_11-52-52.jpg\")\nhands, img_ = detector.findHands(img.copy())  # with draw","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:02:28.767507Z","iopub.execute_input":"2023-09-05T02:02:28.767879Z","iopub.status.idle":"2023-09-05T02:02:28.884465Z","shell.execute_reply.started":"2023-09-05T02:02:28.767849Z","shell.execute_reply":"2023-09-05T02:02:28.882214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(img_)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T05:43:01.304592Z","iopub.execute_input":"2023-09-04T05:43:01.305020Z","iopub.status.idle":"2023-09-04T05:43:01.585207Z","shell.execute_reply.started":"2023-09-04T05:43:01.304982Z","shell.execute_reply":"2023-09-04T05:43:01.584294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting Data","metadata":{}},{"cell_type":"code","source":"files=os.listdir(path)\n# list of files in path\n# sort path from A-Y\nfiles.sort ()\n\n# print to see list\nprint(files)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:19:54.493488Z","iopub.execute_input":"2023-09-05T02:19:54.494558Z","iopub.status.idle":"2023-09-05T02:19:54.502463Z","shell.execute_reply.started":"2023-09-05T02:19:54.494512Z","shell.execute_reply":"2023-09-05T02:19:54.501303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# create list of image and label\nimage_array = []\nlabel_array = []\n\n# Loop through each file in files\nfor i in range(len(files)):\n    # List of images in each folder\n    sub_files = os.listdir(path + \"/\" + files[i])\n    \n    # Let's check the length of each folder\n    #print(len(sub_files))\n    \n    # Loop through each sub folder\n    for j in range(len(sub_files)):\n        # Path of each image\n        # Example: imagepro/A/image_name.jpg\n        file_path = path + \"/\" + files[i] + \"/\" + sub_files[j]\n        \n        # Read each image\n        image = cv2.imread(file_path)\n        \n        # Resize image to 96x96\n        image = cv2.resize(image, (96, 96))\n        \n        # Convert BGR image to RGB image\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        image_array.append(image)\n        label_array.append(i)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:20:00.767434Z","iopub.execute_input":"2023-09-05T02:20:00.767820Z","iopub.status.idle":"2023-09-05T02:20:00.848432Z","shell.execute_reply.started":"2023-09-05T02:20:00.767791Z","shell.execute_reply":"2023-09-05T02:20:00.847112Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert list to array\nimage_array = np.array(image_array)\nlabel_array = np.array(label_array, dtype=\"float\")\n\n# split the dataset into test and train\n#from sklearn.model_selection inport train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-09-05T01:16:13.484018Z","iopub.execute_input":"2023-09-05T01:16:13.484751Z","iopub.status.idle":"2023-09-05T01:16:13.599143Z","shell.execute_reply.started":"2023-09-05T01:16:13.484714Z","shell.execute_reply":"2023-09-05T01:16:13.598097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##---New Data loading method\n#main_dir = \"/kaggle/working/ImagePro\"\nmain_dir = \"/kaggle/input/myimagepro/myImagePro\"\nuniq_labels = sorted(os.listdir(main_dir))\n#print(uniq_labels)\nlab_dic={}\nfor v,k in enumerate(uniq_labels):\n    lab_dic[k.lower()] = v\n#lab_dic[\"0\"] = 40\nprint(lab_dic)\n\ndef file_gen(directory):\n    images = []\n    labels = []\n    #directory = train_dir2\n    train_files = sorted(os.listdir(directory))\n    #print(train_files)\n    for label in train_files:\n        for idx, image_nm in enumerate(os.listdir(directory + \"/\" + label)):\n            if idx in range(200):\n               filepath = directory + \"/\" + label + \"/\" + image_nm\n               #print(image_nm)\n               #print(idx)\n               try: \n                    img = cv2.resize(cv2.imread(filepath), (96,96))\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n               except: \n                    continue               \n               images.append(img)\n               #plt.imshow(image)\n               tag = lab_dic[label.lower()]\n               labels.append(tag)\n               #print(tag)\n        \n    return images, labels ","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:24:58.053868Z","iopub.execute_input":"2023-09-05T02:24:58.054293Z","iopub.status.idle":"2023-09-05T02:24:58.068916Z","shell.execute_reply.started":"2023-09-05T02:24:58.054260Z","shell.execute_reply":"2023-09-05T02:24:58.067731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detector = HandDetector(detectionCon=0.8, maxHands=2)\n'''\n##---New Data loading method\n#main_dir = \"/kaggle/working/ImagePro\"\nmain_dir = \"/kaggle/input/sign-language-dataset/ImagePro\"\nuniq_labels = sorted(os.listdir(main_dir))\n#print(uniq_labels)\nlab_dic={}\nfor v,k in enumerate(uniq_labels):\n    lab_dic[k.lower()] = v\nlab_dic[\"0\"] = 40\nprint(lab_dic)\n\n'''\ndef file_gen_(directory):\n    images = []\n    labels = []\n    #directory = train_dir2\n    train_files = sorted(os.listdir(directory))\n    #print(train_files)\n    for label in train_files:\n        if label == \"Blank\":\n            continue\n        if label == \"10\":\n            continue\n        for idx, image_nm in enumerate(os.listdir(directory + \"/\" + label)):            \n            if idx in range(10):\n               filepath = directory + \"/\" + label + \"/\" + image_nm\n               #print(image_nm)\n               #print(idx)\n               try: \n                    img = cv2.imread(filepath)\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                    hands, img_ = detector.findHands(img.copy())  # with draw\n               except: \n                    continue\n               if hands:\n                    print(\"image detected\")\n                    hand = hands[0]\n                    x1, y1 = hand[\"bbox\"][0], hand[\"bbox\"][1]\n                    x2, y2 = x1 + hand[\"bbox\"][2], y1 + hand[\"bbox\"][3]\n                    padding = 50  # adjust this value to increase/decrease padding\n                    x1 -= padding\n                    y1 -= padding\n                    x2 += padding\n                    y2 += padding\n                    cropped_img = img[max(0, y1):min(y2, img.shape[0]), max(0, x1):min(x2, img.shape[1])]    \n                    cropped_img = cv2.resize(cropped_img, (64,64))\n                    #cropped_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n                    tag = lab_dic[label.lower()]\n                    images.append(cropped_img)\n                    #plt.imshow(image)\n                    labels.append(tag)\n                    #print(tag)\n               else:\n                    print(\"no image detected\")\n    return images, labels ","metadata":{"execution":{"iopub.status.busy":"2023-09-04T07:24:12.640078Z","iopub.execute_input":"2023-09-04T07:24:12.640479Z","iopub.status.idle":"2023-09-04T07:24:12.663844Z","shell.execute_reply.started":"2023-09-04T07:24:12.640447Z","shell.execute_reply":"2023-09-04T07:24:12.661916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a data list\ndata_list = []\nlabel_list = []\n\n\n#read from dir1\n#train_dir1 = \"/kaggle/working/ImagePro\"\ntrain_dir2 = \"/kaggle/input/myimagepro/myImagePro\"\n#images_1, labels_1 = file_gen(train_dir1)\nimages_2, labels_2 = file_gen(train_dir2)\n#images_4, labels_4 = file_gen(train_dir4)\n\n#data_list.extend(images_1)\n#label_list.extend(labels_1)\ndata_list.extend(images_2)\nlabel_list.extend(labels_2)\n#data_list.extend(images_4)\n#label_list.extend(labels_4)\n\n#del(images_1)\n#del(labels_1)\ndel(images_2)\ndel(labels_2)\n#del(images_4)\n#del(labels_4)\n\nimage_array = np.asarray(data_list)\nlabel_array = np.asarray(label_list)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:25:10.476750Z","iopub.execute_input":"2023-09-05T02:25:10.477154Z","iopub.status.idle":"2023-09-05T02:25:15.828654Z","shell.execute_reply.started":"2023-09-05T02:25:10.477123Z","shell.execute_reply":"2023-09-05T02:25:15.827226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##____for test\n\n\ndata_list_test = []\nlabel_list_test = []\n\n\n#read from dir1\ntest_dir1 = \"/kaggle/input/synthetic-asl-numbers/Train_Nums\"\n#images_test, labels_test = file_gen(test_dir1)\nimages_test, labels_test = file_gen_(test_dir1)\n\ndata_list_test.extend(images_test)\nlabel_list_test.extend(labels_test)\n\ndel(images_test)\ndel(labels_test)\n\nimage_array_test = np.asarray(data_list_test)\nlabel_array_test = np.asarray(label_list_test)\n\ny_cat_test = keras.utils.to_categorical(label_array_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T07:24:21.670334Z","iopub.execute_input":"2023-09-04T07:24:21.670692Z","iopub.status.idle":"2023-09-04T07:24:25.312739Z","shell.execute_reply.started":"2023-09-04T07:24:21.670663Z","shell.execute_reply":"2023-09-04T07:24:25.311634Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(image_array_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T07:04:39.899692Z","iopub.execute_input":"2023-09-04T07:04:39.900631Z","iopub.status.idle":"2023-09-04T07:04:39.908295Z","shell.execute_reply.started":"2023-09-04T07:04:39.900585Z","shell.execute_reply":"2023-09-04T07:04:39.907294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_array_test[5]","metadata":{"execution":{"iopub.status.busy":"2023-09-04T07:21:54.471162Z","iopub.execute_input":"2023-09-04T07:21:54.471588Z","iopub.status.idle":"2023-09-04T07:21:54.482058Z","shell.execute_reply.started":"2023-09-04T07:21:54.471556Z","shell.execute_reply":"2023-09-04T07:21:54.480940Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(image_array_test[80])","metadata":{"execution":{"iopub.status.busy":"2023-09-04T07:24:34.674475Z","iopub.execute_input":"2023-09-04T07:24:34.674827Z","iopub.status.idle":"2023-09-04T07:24:34.935163Z","shell.execute_reply.started":"2023-09-04T07:24:34.674799Z","shell.execute_reply":"2023-09-04T07:24:34.934225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output spliting size\nX_train, X_test, y_train, y_test = train_test_split(image_array, label_array, test_size=0.15)\n\n# X_train will have 85% of images\n# X_test will have 15% of inages","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:25:18.073170Z","iopub.execute_input":"2023-09-05T02:25:18.073539Z","iopub.status.idle":"2023-09-05T02:25:18.085454Z","shell.execute_reply.started":"2023-09-05T02:25:18.073511Z","shell.execute_reply":"2023-09-05T02:25:18.084282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T01:16:58.521707Z","iopub.execute_input":"2023-09-05T01:16:58.522086Z","iopub.status.idle":"2023-09-05T01:16:58.529414Z","shell.execute_reply.started":"2023-09-05T01:16:58.522034Z","shell.execute_reply":"2023-09-05T01:16:58.528288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del image_array\ndel label_array","metadata":{"execution":{"iopub.status.busy":"2023-08-31T06:26:36.951357Z","iopub.execute_input":"2023-08-31T06:26:36.952363Z","iopub.status.idle":"2023-08-31T06:26:36.957776Z","shell.execute_reply.started":"2023-08-31T06:26:36.952317Z","shell.execute_reply":"2023-08-31T06:26:36.956825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**converting to categorical for vgg16**","metadata":{}},{"cell_type":"code","source":"import keras\n\ny_train_cat = keras.utils.to_categorical(y_train)\ny_test_cat = keras.utils.to_categorical(y_test)\n#y_eval_cat = keras.utils.to_categorical(y_eval)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:25:24.767931Z","iopub.execute_input":"2023-09-05T02:25:24.768637Z","iopub.status.idle":"2023-09-05T02:25:24.774528Z","shell.execute_reply.started":"2023-09-05T02:25:24.768601Z","shell.execute_reply":"2023-09-05T02:25:24.773254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_cat.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:25:29.342154Z","iopub.execute_input":"2023-09-05T02:25:29.342533Z","iopub.status.idle":"2023-09-05T02:25:29.349787Z","shell.execute_reply.started":"2023-09-05T02:25:29.342504Z","shell.execute_reply":"2023-09-05T02:25:29.348792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fist model (EfficientNetDB)**","metadata":{}},{"cell_type":"code","source":"from keras import layers, callbacks, utils, applications, optimizers\nfrom keras.models import Sequential, Model, load_model\n\nmodel = Sequential()\n\n# Add pretrained models to Sequential model\n# I will use EfficientNetB0 pretrained model. You can try a different model.\npretrained_model = tf.keras.applications.EfficientNetB0(input_shape=(96,96,3), include_top=False)\n\n#don't train existing weights for vgg16\n#for layer in classifier_vgg16.layers:\n    #layer.trainable = False\n\nmodel.add(pretrained_model)\n\n# Add pooling to the model\nmodel.add(layers.GlobalAveragePooling2D())\n\n# Add dropout to the model\n# We add dropout to increase accuracy by reducing overfitting\nmodel.add(layers.Dropout(0.3))\n\n# Finally, add a dense layer as the output\nmodel.add(layers.Dense(1))\n\n# Print the model summary\nmodel.summary()\nmodel.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mae\"])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:59:25.938401Z","iopub.execute_input":"2023-09-05T02:59:25.938785Z","iopub.status.idle":"2023-09-05T02:59:29.624889Z","shell.execute_reply.started":"2023-09-05T02:59:25.938755Z","shell.execute_reply":"2023-09-05T02:59:29.623958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create a checkpoint to save the best model\ncheckpoint_path = \"trained_model/model\"\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path,\n    monitor=\"val_mae\",\n    mode=\"auto\",\n    save_best_only=True,\n    save_weights_only=True\n)\n\n# Learning rate reducer\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    patience=5,\n    factor=0.9,\n    monitor=\"val_mae\",\n    mode=\"auto\",\n    cooldown=0,\n    verbose=1,\n    min_lr=1e-6\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:59:33.363319Z","iopub.execute_input":"2023-09-05T02:59:33.363695Z","iopub.status.idle":"2023-09-05T02:59:33.370265Z","shell.execute_reply.started":"2023-09-05T02:59:33.363665Z","shell.execute_reply":"2023-09-05T02:59:33.369114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n# Define the CNN model3\nmodel3 = tf.keras.Sequential()\n\n# Convolutional layers\nmodel3.add(Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 3)))\nmodel3.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel3.add(Conv2D(64, (3, 3), activation='relu'))\nmodel3.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel3.add(Conv2D(128, (3, 3), activation='relu'))\nmodel3.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flatten the feature maps\nmodel3.add(Flatten())\n\n# Dense (fully connected) layers\nmodel3.add(Dense(256, activation='relu'))\nmodel3.add(Dropout(0.5))  # Dropout for regularization\n\nmodel3.add(Dense(128, activation='relu'))\nmodel3.add(Dropout(0.5))  # Dropout for regularization\n\n# Output layer\nmodel3.add(Dense(10, activation='softmax'))  # 10 output classes\n\n# Compile the model3\nmodel3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Print the model3 summary\nmodel3.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T03:27:43.433001Z","iopub.execute_input":"2023-09-05T03:27:43.433976Z","iopub.status.idle":"2023-09-05T03:27:43.571900Z","shell.execute_reply.started":"2023-09-05T03:27:43.433940Z","shell.execute_reply":"2023-09-05T03:27:43.571110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model 2 (VGG16)**","metadata":{}},{"cell_type":"code","source":"#Initialising vgg16 \nclassifier_vgg16 = VGG16(input_shape= (96,96,3),include_top=False,weights='imagenet')\n\n#don't train existing weights for vgg16\nfor layer in classifier_vgg16.layers:\n    layer.trainable = False\n\nclassifier1 = classifier_vgg16.output#head mode\nclassifier1 = Flatten()(classifier1)#adding layer of flatten\nclassifier1 = Dense(units=256, activation='relu')(classifier1)\nclassifier1 = Dropout(0.6)(classifier1)\nclassifier1 = Dense(units=10, activation='softmax')(classifier1)\n\nmodel2 = Model(inputs = classifier_vgg16.input , outputs = classifier1)\nmodel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel2.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:30:24.973969Z","iopub.execute_input":"2023-09-05T02:30:24.974699Z","iopub.status.idle":"2023-09-05T02:30:25.373940Z","shell.execute_reply.started":"2023-09-05T02:30:24.974662Z","shell.execute_reply":"2023-09-05T02:30:25.372360Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_cat.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-31T06:30:59.928267Z","iopub.execute_input":"2023-08-31T06:30:59.929289Z","iopub.status.idle":"2023-08-31T06:30:59.936440Z","shell.execute_reply.started":"2023-08-31T06:30:59.929237Z","shell.execute_reply":"2023-08-31T06:30:59.935270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start training model\nepochs = 50\nbatch_size = 32\n# select batch size according to your graphic card\n# X_train, X_test, Y_train, Y_test\n#history = model2.fit(\nhistory = model.fit(\n    X_train,\n    y_train,\n    #validation_data=(X_test, y_test),\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[model_checkpoint, reduce_lr]\n)\n# before training you can delete image_array and label_array to increase ram memory\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T03:02:28.704400Z","iopub.execute_input":"2023-09-05T03:02:28.704788Z","iopub.status.idle":"2023-09-05T03:03:50.708632Z","shell.execute_reply.started":"2023-09-05T03:02:28.704751Z","shell.execute_reply":"2023-09-05T03:03:50.707559Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start training model\nepochs = 50\nbatch_size = 32\n# select batch size according to your graphic card\n# X_train, X_test, Y_train, Y_test\nhistory = model2.fit(\n    X_train,\n    y_train_cat,\n    #validation_data=(X_test, y_test_cat),\n    batch_size=batch_size,\n    epochs=epochs,\n    #callbacks=[model_checkpoint, reduce_lr]\n)\n# before training you can delete image_array and label_array to increase ram memory\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:30:40.716270Z","iopub.execute_input":"2023-09-05T02:30:40.717330Z","iopub.status.idle":"2023-09-05T02:31:16.341968Z","shell.execute_reply.started":"2023-09-05T02:30:40.717292Z","shell.execute_reply":"2023-09-05T02:31:16.340959Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start training model\nepochs = 5\nbatch_size = 32\n# select batch size according to your graphic card\n# X_train, X_test, Y_train, Y_test\nhistory = model3.fit(\n    X_train,\n    y_train_cat,\n    #validation_data=(X_test, y_test_cat),\n    batch_size=batch_size,\n    epochs=epochs,\n    #callbacks=[model_checkpoint, reduce_lr]\n)\n# before training you can delete image_array and label_array to increase ram memory\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T03:27:48.984345Z","iopub.execute_input":"2023-09-05T03:27:48.984723Z","iopub.status.idle":"2023-09-05T03:27:53.381960Z","shell.execute_reply.started":"2023-09-05T03:27:48.984692Z","shell.execute_reply":"2023-09-05T03:27:53.380748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(y_train_cat.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T03:24:48.957629Z","iopub.execute_input":"2023-09-05T03:24:48.957992Z","iopub.status.idle":"2023-09-05T03:24:48.964318Z","shell.execute_reply.started":"2023-09-05T03:24:48.957961Z","shell.execute_reply":"2023-09-05T03:24:48.963224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation on normal test data","metadata":{}},{"cell_type":"code","source":"#score = model.evaluate(x = X_test, y = y_test, verbose = 0)\n#print('Accuracy for test images:', round(score[1]*100, 3), '%')\nscore = model3.evaluate(x = X_test, y = y_test_cat, verbose = 0)\nprint('Accuracy for evaluation images:', round(score[1]*100, 3), '%')","metadata":{"execution":{"iopub.status.busy":"2023-09-05T03:28:17.490685Z","iopub.execute_input":"2023-09-05T03:28:17.491091Z","iopub.status.idle":"2023-09-05T03:28:18.003110Z","shell.execute_reply.started":"2023-09-05T03:28:17.491033Z","shell.execute_reply":"2023-09-05T03:28:18.002062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation on a differrent test data","metadata":{}},{"cell_type":"code","source":"#score = model.evaluate(x = image_array_test, y = y_cat_test, verbose = 0)\n#print('Accuracy for test images:', round(score[1]*100, 3), '%')\nscore = model2.evaluate(x = image_array_test, y = y_cat_eval, verbose = 0)\nprint('Accuracy for evaluation images:', round(score[1]*100, 3), '%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(X_test, batch_size=2)\nprint(predictions[:10])\nprint(y_test[:10])","metadata":{"execution":{"iopub.status.busy":"2023-08-31T01:57:18.540445Z","iopub.status.idle":"2023-08-31T01:57:18.541162Z","shell.execute_reply.started":"2023-08-31T01:57:18.540897Z","shell.execute_reply":"2023-08-31T01:57:18.540921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model2.predict(X_test, batch_size=2)\nprint(predictions[:10])\nprint(y_test[:10])","metadata":{"execution":{"iopub.status.busy":"2023-08-31T06:36:29.208778Z","iopub.execute_input":"2023-08-31T06:36:29.209159Z","iopub.status.idle":"2023-08-31T06:36:31.739730Z","shell.execute_reply.started":"2023-08-31T06:36:29.209126Z","shell.execute_reply":"2023-08-31T06:36:31.737671Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab_dic","metadata":{"execution":{"iopub.status.busy":"2023-08-31T06:36:46.870925Z","iopub.execute_input":"2023-08-31T06:36:46.871342Z","iopub.status.idle":"2023-08-31T06:36:46.879576Z","shell.execute_reply.started":"2023-08-31T06:36:46.871309Z","shell.execute_reply":"2023-08-31T06:36:46.878560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_array= []\n\n# Read each image\nimage = cv2.imread(\"/kaggle/working/ImagePro/ImagePro/5/2023-08-30_18-27-17.jpg\")\n        \n# Resize image to 96x96\nimage = cv2.resize(image, (96, 96))\n        \n# Convert BGR image to RGB image\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \nimage_array.append(image)\n#label_array.append(i)\n\n# convert list to array\nimage_array = np.array(image_array)\n\npredictions = model3.predict(image_array, batch_size=2)\npred = np.argmax(predictions)\nprint(pred)\nplt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T03:31:00.641876Z","iopub.execute_input":"2023-09-05T03:31:00.642285Z","iopub.status.idle":"2023-09-05T03:31:00.946893Z","shell.execute_reply.started":"2023-09-05T03:31:00.642253Z","shell.execute_reply":"2023-09-05T03:31:00.945926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im = X_test[90]\npredictions = model2.predict(np.array([im]), batch_size=2)\npred = np.argmax(predictions)\nprint(pred)\nplt.imshow(im)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:36:24.355469Z","iopub.execute_input":"2023-09-05T02:36:24.355853Z","iopub.status.idle":"2023-09-05T02:36:24.657924Z","shell.execute_reply.started":"2023-09-05T02:36:24.355822Z","shell.execute_reply":"2023-09-05T02:36:24.656993Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detector = HandDetector(detectionCon=0.8, maxHands=2)\nimage = cv2.imread(\"/kaggle/working/ImagePro/ImagePro/2/2023-08-30_18-21-32.jpg\")\nhands, img = detector.findHands(image.copy())  # with draw\n\nif hands:\n    print(\"image detected\")\n    hand = hands[0]\n    x1, y1 = hand[\"bbox\"][0], hand[\"bbox\"][1]\n    x2, y2 = x1 + hand[\"bbox\"][2], y1 + hand[\"bbox\"][3]\n    padding = 50  # adjust this value to increase/decrease padding\n    x1 -= padding\n    y1 -= padding\n    x2 += padding\n    y2 += padding\n    cropped_img = image[max(0, y1):min(y2, img.shape[0]), max(0, x1):min(x2, img.shape[1])]    \n    #cv2_imshow(cropped_img)\nelse:\n    print(\"No image detected\")\n    cropped_img = image\n\nimage_array= []\n\n# Read each image\n#image = cv2.imread(\"/kaggle/input/mydata-alpha-2long/20230830_125715.jpg\")\n\nimage = cropped_img\n# Resize image to 96x96\nimage = cv2.resize(image, (96, 96))\n        \n# Convert BGR image to RGB image\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \nimage_array.append(image)\n#label_array.append(i)\n\n# convert list to array\nimage_array = np.array(image_array)\npredictions = model2.predict(image_array, batch_size=2)\npred = np.argmax(predictions)\nprint(pred)\nplt.imshow(image)\n#plt.imshow(cropped_img)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:46:03.637390Z","iopub.execute_input":"2023-09-05T02:46:03.637790Z","iopub.status.idle":"2023-09-05T02:46:04.041924Z","shell.execute_reply.started":"2023-09-05T02:46:03.637758Z","shell.execute_reply":"2023-09-05T02:46:04.040968Z"},"trusted":true},"execution_count":null,"outputs":[]}]}