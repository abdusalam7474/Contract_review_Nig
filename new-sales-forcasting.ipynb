{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Lib","metadata":{}},{"cell_type":"code","source":"# Basic\nimport numpy as np \nimport pandas as pd\n\n# Visalizaton\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Data Preprosessing\nfrom datetime import date\nimport holidays\n\n# Models\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# Ensemble\nfrom sklearn.ensemble import VotingRegressor\n# Cross-validation\nfrom sklearn.model_selection import cross_val_score\n\n# Ignore warings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s3e19/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s3e19/test.csv')\ncombine=[train,test]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Infromation from data","metadata":{}},{"cell_type":"code","source":"train.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# differnt product present in the dataset\ntrain['product'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['store'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Select the rows where the `country` column is equal to `US`.\nuss_df = train[train['store'] == 'Kaggle Learn']\nus_df = uss_df[uss_df['country'] == 'Spain']\n\nbook = us_df[us_df['product'] == 'Using LLMs to Improve Your Coding']\nbook = book.reset_index(drop=True)\nshoe = us_df[us_df['product'] == 'Using LLMs to Train More LLMs']\nshoe = shoe.reset_index(drop=True)\nring = us_df[us_df['product'] == 'Using LLMs to Write Better']\nring = ring.reset_index(drop=True)\nnecklace = us_df[us_df['product'] == 'Using LLMs to Win More Kaggle Competitions']\nnecklace = necklace.reset_index(drop=True)\npearls = us_df[us_df['product'] == 'Using LLMs to Win Friends and Influence People']\npearls = pearls.reset_index(drop=True)\n\nnew_df = pd.DataFrame()\n\nproducts_df = [book, shoe, ring, necklace, pearls]\nproducts_name = [\"book\", \"shoe\", \"ring\", \"necklace\", \"pearls\"] \nfor i in range(5):\n    new_df[products_name[i]] = products_df[i]['num_sold'].copy()\n    \n# Create a datetime object for the start date\nstart_date = pd.to_datetime('2017-01-01')\n\n# Create a new column in the DataFrame called `date`\nnew_df['date'] = pd.Series(dtype='datetime64[ns]')\nnew_order = ['date'] + list(new_df.columns[:-1])\nnew_df = new_df.reindex(columns=new_order)\n\nfor i in range(len(new_df)):\n    new_df.loc[i, 'date'] = start_date\n    start_date += pd.Timedelta(days=1)\n\nnew_df.head()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stop here","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({'col1': [1, 2, 3, 4, 5]})\n\n# Create a new column in the DataFrame called `date`\ndf['date'] = pd.to_datetime('2017-01-01') + pd.to_timedelta(range(len(df)), unit='D')\n\n# Set the date column as the first column\ndf = df[['date'] + df.columns.tolist()[1:]]\n\n# Print the DataFrame\nprint(df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the rows where the `country` column is equal to `US`.\nuss_df = train[train['store'] == 'Kaggle Learn']\nus_df = uss_df[uss_df['country'] == 'Spain']\n\nbook = us_df[us_df['product'] == 'Using LLMs to Improve Your Coding']\nshoe = us_df[us_df['product'] == 'Using LLMs to Train More LLMs']\nring = us_df[us_df['product'] == 'Using LLMs to Write Better']\nnecklace = us_df[us_df['product'] == 'Using LLMs to Win More Kaggle Competitions']\npearls = us_df[us_df['product'] == 'Using LLMs to Write Better']\n\nnew_df = pd.DataFrame()\nnew_df[\"wet\"] = shoe['num_sold'].copy()\n#new_df[\"wet2\"] = book['num_sold'].copy()\nnew_df[\"wet2\"] = 0\nnew_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df[\"wet2\"] = book['num_sold'].copy()\nnew_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shoe.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df[\"wet\"] = shoe['num_sold'].copy()\nnew_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new dataframe.\nnew_df = pd.DataFrame()\n# Copy the `country` column from the `train` dataframe to the `new_df` dataframe.\nnew_df['country_cod'] = train['country'].copy()\n# Rename the column in the `new_df` dataframe to `country_code`.\n#new_df.rename(columns={'country_code': 'country'}, inplace=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uss_df['country'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"us_df = uss_df[uss_df['country'] == 'spain']\nus_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# create sample dataframes\ndf1 = pd.DataFrame({'date': ['2017-01-2', '2017-01-3', '2017-01-2', '2017-01-3'],\n                    'p': ['gh', 'gh', 'ml', 'ml'],\n                    'v': [12, 45, 21, 32]})\n\ndf2 = pd.DataFrame({'date': ['2017-01-2', '2017-01-3'],\n                    'gh': [None, None],\n                    'ml': [None, None]})\n\n# merge dataframes on date column\nmerged_df = pd.merge(df1, df2, on='date')\n\n# pivot table to reshape dataframe\npivoted_df = merged_df.pivot_table(index='date', columns='p', values='v')\n\n# sort dataframe by date\nsorted_df = pivoted_df.sort_index()\n\nprint(sorted_df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge dataframes on date column\nmerged_df = pd.merge(p1_df, p1_df, on='date')\n\n# pivot table to reshape dataframe\npivoted_df = merged_df.pivot_table(index='date', columns='p', values='v')\n\n# sort dataframe by date\nsorted_df = pivoted_df.sort_index()\n\nprint(sorted_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# differnt stores present in data\ntrain.store.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# differnt product present in the dataset\ntrain['product'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Date column changes","metadata":{}},{"cell_type":"code","source":"for data in combine:\n    data['date']=pd.to_datetime(data.date)\n    \n    data['month']=data['date'].dt.month\n    data['day'] = data['date'].dt.day\n    data['day_of_week']=data.date.dt.day_of_week\n    data['year'] = data['date'].dt.year\n    \n    data['quarter'] = data['date'].dt.quarter\n    data['weekofyear'] = data['date'].dt.isocalendar().week \n    data[\"dayofyear\"] = data['date'].dt.dayofyear\n    data[\"is_month_end\"] = data[\"date\"].dt.is_month_end\n    data[\"is_month_start\"] = data[\"date\"].dt.is_month_start\n    data[\"is_year_end\"] = data[\"date\"].dt.is_year_end\n    data[\"is_year_start\"] = data[\"date\"].dt.is_year_start\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Num_sold per year","metadata":{}},{"cell_type":"code","source":"# grouping by year to see num_sold\na = train.groupby('year')['num_sold'].sum().reset_index() \na","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot for num_sold vs year\nplt.figure(figsize=(14,5))\nsns.set_style('darkgrid')\nsns.lineplot(data=a,x='year',y='num_sold',markers='<')\nplt.tight_layout()\nplt.title('Time series Graph for num_sold yearwise')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Num_sold per Month","metadata":{}},{"cell_type":"code","source":"months={1:'January',2:'February',3:'March',4:'April',5:'May',6:'June',7:'July',8:'August',9:'September',10:'October',11:'November',12:'December'}\ndf = train.groupby('month')['num_sold'].sum().reset_index()\ndf['month'] = df['month'].map(months)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot for num_sold per month\nplt.figure(figsize=(14,5))\nsns.lineplot(data=df,x='month',y='num_sold')\nplt.title('Time Series Graph for num_sold per month')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Num_sold per Day","metadata":{}},{"cell_type":"code","source":"days = {1:'Tuesday',0:'Monday',3:'Thursday',2:'Wednesday',4: 'Friday',5:'Saturday',6:'Sunday'}\nday = train.groupby('day_of_week')['num_sold'].sum().reset_index()\nday['day_of_week'] = day['day_of_week'].map(days)\nday","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,5))\nsns.lineplot(data=day,x='day_of_week',y='num_sold')\nplt.title('Time Series Graph for num_sold per day')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Num_sold per Country","metadata":{}},{"cell_type":"code","source":"count = train.groupby('country')['num_sold'].sum().reset_index()\ncount","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,5))\nsns.barplot(data = count, x='country', y='num_sold')\nplt.title('Num_sold per Country')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Num_sold for every store","metadata":{}},{"cell_type":"code","source":"store = train.groupby('store')['num_sold'].sum().reset_index()\nstore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.pie(data=store,labels='store',x='num_sold',autopct='%.0f%%')\nplt.title('Stores with higest num_sold')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Num_sold per product","metadata":{}},{"cell_type":"code","source":"pro = train.groupby('product')['num_sold'].sum().reset_index()\npro","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.barplot(data=pro,x='product',y='num_sold')\nplt.title('product with highest num_sold')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Enginering","metadata":{}},{"cell_type":"code","source":"# Country column changes to ordinal values\nfor data in combine:\n    list={'Argentina':1,'Canada':2,'Estonia':3,'Japan':4,'Spain':5 }\n    data['country']=data['country'].map(list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store column changes to ordinal value \nfor data in combine:\n    list={'Kaggle Learn':1,'Kaggle Store':2,'Kagglazon':3}\n    data['store']=data.store.map(list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Product changes to ordinal value \nfor data in combine:\n    list={'Using LLMs to Improve Your Coding':1,'Using LLMs to Train More LLMs':2,'Using LLMs to Win Friends and Influence People':3,'Using LLMs to Win More Kaggle Competitions':4 ,'Using LLMs to Write Better':5} \n    data['product']=data['product'].map(list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## New features","metadata":{}},{"cell_type":"code","source":"# Crate weekend column, weekend = 1, weekday = 0\nfor df in combine:\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df.loc[df['day_of_week'] > 4, 'weekend'] = 1\n    df.loc[df['day_of_week'] <= 4, 'weekend'] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Sunday Column, Sunday = 1, others = 0\nfor df in combine:    \n    df.loc[df['day_of_week'] == 6, 'sunday'] = 1\n    df.loc[df['day_of_week'] != 6, 'sunday'] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Fourier Features\nfor df in combine:   \n    df['month_sin'] = np.sin(2*np.pi*df['month']/12)\n    df['month_cos'] = np.cos(2*np.pi*df['month']/12)\n    df['day_sin'] = np.sin(2*np.pi*df['day']/31)\n    df['day_cos'] = np.cos(2*np.pi*df['day']/31)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Co-vid Feature\nstart_date = pd.to_datetime('2020-03-01')\nend_date = pd.to_datetime('2021-01-01')\n\nfor df in combine:\n    df['covid'] = df['date'].apply(lambda x: 1 if start_date <= x <= end_date else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store_product feature\nfor data in combine:\n    data['store_product'] = data['store']+ data['product']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Holiday Feature  ","metadata":{}},{"cell_type":"code","source":"# creating list of holidays for each country for years mentioned in data.\nyears = data[\"year\"].unique().tolist()\n\nholiday_AR = holidays.CountryHoliday(\"AR\", years=years)\nholiday_CA = holidays.CountryHoliday(\"CA\", years=years)\nholiday_EE = holidays.CountryHoliday(\"EE\", years=years)\nholiday_JP = holidays.CountryHoliday(\"JP\", years=years)\nholiday_ES = holidays.CountryHoliday(\"ES\", years=years)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Checking a date is holiday or not for each country.\ndf_holidays = pd.DataFrame(columns=['date', 'AR', 'CA', 'EE', 'JP', 'ES'])\n\nfor date in pd.date_range(start='2017-01-01', end='2022-12-31'):\n    ar = 1 if date in holiday_AR else 0\n    ca = 1 if date in holiday_CA else 0\n    ee = 1 if date in holiday_EE else 0\n    jp = 1 if date in holiday_JP else 0\n    es = 1 if date in holiday_ES else 0\n    \n    df_holidays = df_holidays.append({\"date\": date, \"AR\": ar, \"CA\": ca, \"EE\": ee, \"JP\": jp, \"ES\": es}, ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#__introduced to replace above\n'''\n# Create a DataFrame to store the holidays\ndf_holidays = pd.DataFrame(columns=['date', 'AR', 'CA', 'EE', 'JP', 'ES'])\n\n# Iterate over the dates in the range 2017-01-01 to 2022-12-31\nfor date in pd.date_range(start='2017-01-01', end='2022-12-31'):\n\n    # Create a dictionary containing the holiday status for each country\n    holiday_dict = {\n        'AR': 1 if date in holiday_AR else 0,\n        'CA': 1 if date is holiday_CA else 0,\n        'EE': 1 if date is holiday_EE else 0,\n        'JP': 1 if date is holiday_JP else 0,\n        'ES': 1 if date is holiday_ES else 0\n    }\n\n    # Append the dictionary to the DataFrame using the `concat()` method\n    df_holidays = pd.concat([df_holidays, pd.DataFrame.from_dict(holiday_dict, orient='index')], ignore_index=True)\n\n# Print the DataFrame\nprint(df_holidays)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# New feature addition for each country\nfor data in combine:\n    data['is_holiday_Argentina'] = data['date'].map(df_holidays.set_index('date')['AR']).astype(int)\n    data['is_holiday_Canada'] = data['date'].map(df_holidays.set_index('date')['CA']).astype(int)\n    data['is_holiday_Estonia'] = data['date'].map(df_holidays.set_index('date')['EE']).astype(int)\n    data['is_holiday_Japan'] = data['date'].map(df_holidays.set_index('date')['JP']).astype(int)\n    data['is_holiday_Spain'] = data['date'].map(df_holidays.set_index('date')['ES']).astype(int)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation between features\nsns.heatmap(train.corr())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Droping the id and date column.\nfor data in combine:\n    data.drop(columns=['id','date'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# changing the data type of weekofyear column.\nfor data in combine:\n     data['weekofyear'] = data['weekofyear'].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking columns before staring the training\ntest.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"code","source":"X_train=train.drop(columns='num_sold')\ny_train=train.num_sold\nX_test = test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standard Scaling","metadata":{}},{"cell_type":"code","source":"ss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluvation method ","metadata":{}},{"cell_type":"code","source":"def cv_score(score):\n    rmse = np.sqrt(-score) # -score because we are using \"neg_mean_squared_error\" as our metric\n    return (rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"lm = LinearRegression()\n\nlm.fit(X_train,y_train)\n\npred_lm = np.round(lm.predict(X_test))\n\nlm_score = round(lm.score(X_train,y_train)*100,2)\nprint(lm_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"rf = RandomForestRegressor()\n\nrf.fit(X_train,y_train)\n\npred_rf = rf.predict(X_test)\n\nrf_score = round(rf.score(X_train,y_train)*100, 2)\nrf_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Boosting Regressor","metadata":{}},{"cell_type":"code","source":"gr_best =  {'learning_rate': 0.13119348735666758, \n            'max_depth': 8.0, \n            'n_estimators': 150.0}\n\n\ngb = GradientBoostingRegressor(n_estimators=int(gr_best['n_estimators']), \n                               max_depth=int(gr_best['max_depth']),\n                               learning_rate=gr_best['learning_rate'],\n                               loss='absolute_error',random_state=156)\n\ngb.fit(X_train,y_train)\n\npred_gb = gb.predict(X_test)\n\ngb_score = round(gb.score(X_train,y_train)*100, 2)\n\ngb_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ada Boost Regressor","metadata":{}},{"cell_type":"code","source":"ad = AdaBoostRegressor()\n\nad.fit(X_train,y_train)\n\npred_ad = ad.predict(X_test)\n\nad_score = round(ad.score(X_train,y_train)*100, 2)\n\nad_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGB Regressor","metadata":{}},{"cell_type":"code","source":"xgb = XGBRegressor()\n\nxgb.fit(X_train,y_train)\n\npred_xgb = xgb.predict(X_test)\n\nxb_score = round(xgb.score(X_train,y_train)*100,2)\n\nxb_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGMB Regressor","metadata":{}},{"cell_type":"code","source":"lgbm_best = {'learning_rate': 0.13431395434407706, \n             'min_child_samples': 70.0, \n             'n_estimators': 200.0, \n             'num_leaves': 80.0}\n\nlgmb = LGBMRegressor(learning_rate=lgbm_best['learning_rate'], \n                        min_child_samples=int(lgbm_best['min_child_samples']),\n                        n_estimators=int(lgbm_best['n_estimators']),\n                        num_leaves=int(lgbm_best['num_leaves']), \n                        objective='mae',random_state=156)\n\nlgmb.fit(X_train,y_train)\n\npred_lgb=lgmb.predict(X_test)\n\nscore_lgb=round(lgmb.score(X_train,y_train)*100, 2)\n\nscore_lgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cat Boost Regressor","metadata":{}},{"cell_type":"code","source":"cat_best = {'bootstrap_type': 0, \n            'depth': 7, \n            'iterations': 745, \n            'l2_leaf_reg': 3, \n            'learning_rate': 0.04964042259047742}\n\n\ncat = CatBoostRegressor(learning_rate=0.089075,\n                            iterations=cat_best['iterations'],\n                            l2_leaf_reg=cat_best['l2_leaf_reg'],\n                            depth=cat_best['depth'],\n                            bootstrap_type='Bayesian',\n                            objective='MAE',\n                            silent=True,\n                            random_state=156)\n\ncat.fit(X_train,y_train)\n\npred_cat = cat.predict(X_test)\n\nscore_cat = round(cat.score(X_train,y_train)*100, 2)\n\nscore_cat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluvation Table","metadata":{}},{"cell_type":"code","source":"modles = pd.DataFrame({'Model':['Linear Regression','Random Forest Regressor','Gradient Boosting Regressor','Ada Boost Regressor','Xgboost','LGBMRegressor',' Cat Boost Regressor'], 'score':[lm_score,rf_score,gb_score,ad_score,xb_score,score_lgb,score_cat]})\nmodles.sort_values(by='score',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,5))\nsns.barplot(data=modles,x='Model',y='score')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble of modles","metadata":{}},{"cell_type":"code","source":"# Instantiate the Regressor\nvoting_reg = VotingRegressor(\n            estimators=[('rf',rf),  \n                        ('cb_reg',cat),\n                        ('gb',gb ),\n                        ('lgm',lgmb),\n                        ('cat',cat)])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate cross validation score for the Voting regresssor\nscores = cross_val_score(voting_reg, X_train, y_train, scoring=\"neg_mean_squared_error\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Given that we have negative MSE score, lets first get the root squares to get RMSE's and then calculate the mean\nvoting_reg_score = np.sqrt(-scores)\n\n# Calc mean for RMSE\nprint(voting_reg_score.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting the voting regressor on the entire training dataset\nvoting_reg.fit(X_train, y_train)\n\n# Predict on test set\npred = voting_reg.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'Argentina':1,'Canada':2,'Estonia':3,'Japan':4,'Spain':5\n# Using diifernt sales for each country to find a frequency.\nforecast_probed = pd.Series(np.round(pred), index=test.index, copy=True)\nprobed_map = {\n    1: 3.5,\n    5 : 1.5,\n    4 : 1.4,\n    3 : 1.7,\n    2 : 0.8,\n}\n\nforecast_probed = np.round(forecast_probed * test['country'].replace(probed_map) , 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# import submission file\nsubmission = pd.read_csv('/kaggle/input/playground-series-s3e19/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# assign predicated value\nsubmission['num_sold'] = forecast_probed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting to csv\nsubmission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}