{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11650,"sourceType":"datasetVersion","datasetId":8327},{"sourceId":59255,"sourceType":"datasetVersion","datasetId":38891},{"sourceId":239192,"sourceType":"datasetVersion","datasetId":100982},{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":2510329,"sourceType":"datasetVersion","datasetId":1520310},{"sourceId":4368115,"sourceType":"datasetVersion","datasetId":1152384}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abdusalam7474/sentiment-analyis-ml-dl-dbert?scriptVersionId=173318835\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"### üîµ Installing and importing libraries <a class=\"anchor\"  id=\"a1\"></a>","metadata":{}},{"cell_type":"code","source":"## Install necessary libraries\n\n! pip install -Uqqq seaborn\n! pip install -Uqqq squarify\n! pip install -Uqqq nltk\n! pip install -Uqqq wordcloud\n! pip install -Uqqq spacy\n! pip install -Uqqq imblearn\n! pip install -Uqqq gensim\n!python -m spacy download en_core_web_sm","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-22T05:14:13.975223Z","iopub.execute_input":"2024-04-22T05:14:13.976001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Importing the necessary libraries \n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imread\nimport requests\nfrom PIL import Image\nfrom collections import Counter\nimport squarify\nfrom tqdm import tqdm\n\nimport string\nimport re\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud ,STOPWORDS\nimport spacy\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\n\nimport tensorflow as tf\nimport shutil\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, GRU,SimpleRNN\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import BatchNormalization\nfrom keras.utils import to_categorical\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n# Load the spaCy English model\nnlp = spacy.load(\"en_core_web_sm\")\nnltk.download('vader_lexicon')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Helper functions\n\ndef add_spines(colour = '#425169', linewidth = 2):\n    \"\"\"\n    Add beautiful spines to you plots\n    \"\"\"\n    ax = plt.gca()\n    ax.spines['top'].set_visible(True)\n    ax.spines['right'].set_visible(True)\n    ax.spines[['bottom', 'left', 'top', 'right']].set_color(colour)\n    ax.spines[['bottom', 'left', 'top', 'right']].set_linewidth(linewidth)\n\ndef roc_auc(predictions,target):\n    '''\n    This methods returns the AUC Score when given the Predictions\n    and Labels\n    '''\n    \n    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n    roc_auc = metrics.auc(fpr, tpr)\n    return roc_auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### üîµ Basic preprocessing and sample tweets <a class=\"anchor\"  id=\"a2\"></a>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/twitter_parsed_dataset.csv\")\ndf = df.drop(['id', 'index'], axis=1)\ndf.rename(columns={'oh_label': 'Label'}, inplace=True)\ndf = df.dropna(subset=['Label'])\n\n# analysis of mentions and tweets\n\n# here we create a function to count the number of hashtags and mentions and then create two columns to store this data\ndef count_symbols(text):\n    hashtag_count = len(re.findall(r'#', text))\n    mention_count = len(re.findall(r'@', text))\n    return hashtag_count, mention_count\n\n# Apply the function to each row in the 'text' column\ndf[['num_hashtags', 'num_mentions']] = df['Text'].apply(lambda x: pd.Series(count_symbols(x)))\n\nprint('\\nDataset shape: ', df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('_________Sample tweets_________')\nsimple_tweet = df[df['Label'] == 0]['Text'].iloc[1]\nprint('\\nSimple tweet: ', simple_tweet)\nsexist_tweet = df[df['Annotation'] == 'sexism']['Text'].iloc[0]\nprint('\\n\\nSexist tweet: ', sexist_tweet)\nracist_tweet = df[df['Annotation'] == 'racism']['Text'].iloc[0]\nprint('\\n\\nRacist tweet: ', racist_tweet)\nprint('\\n_______________________________')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üïµüèª Exploratory Data Analysis <a class=\"anchor\"  id=\"chapter1\"></a>\n\n>We analyse different aspects of the twitter dataset here. We start with simple tasks like understanding the label and Annotation distribution, and slowly progress toward extracting and analysing #hastags, @mentions and finally generate wordclouds of high frequency words according to annotations. ","metadata":{}},{"cell_type":"code","source":"## Basic analysis of labels and annotations\n\nprint('Label distribution')\nprint(df.Label.value_counts())\nprint('-------------------------\\n')\nprint('Annotation distribution')\nprint(df.Annotation.value_counts())\nprint('-------------------------\\n')\nprint('Grouping of Annotation with label')\nprint(df.groupby('Annotation')['Label'].sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen, the text with annotations of anything other than none are all classified as toxic. Hence, it would be a dead giveaway to include it in our dataset. We may try to classify the text with the annotations later\n___","metadata":{}},{"cell_type":"code","source":"def bully_palette(df,column, positions_to_change: list):\n    \"\"\"\n    A function to create grey red palettes according to the inputs\n    You just need to pass in the dataframe and the index of labels to be highlighted in red\n    \"\"\"\n    palette = ['#96898b']*df[column].nunique()\n    new_values = ['#cc253b']*len(positions_to_change)\n    for position, new_value in zip(positions_to_change, new_values):\n        palette[position] = new_value\n    return sns.color_palette(palette)\n    \n# example\n# bully_palette(df,'Label', [0, 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nplt.figure(figsize=(6, 2))\n\nplt.subplot(1, 2, 1)\n# using the bully_pallete function to create a custom pallete\nsns.countplot(x='Label', data=df, palette = bully_palette(df, 'Label', [1]))\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.title('Distribution of Labels')\n# adding spines using the helper function we declared earlier\nadd_spines(linewidth=1)\n\nplt.subplot(1, 2, 2)\nsns.countplot(x='Annotation', data=df, palette = bully_palette(df,'Annotation', [1, 2]))\nplt.xlabel('Annotation')\nplt.ylabel('')\nplt.title('Distribution of Annotation')\nadd_spines(linewidth=1)\n\nplt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.4, hspace=0.3)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is how our highlighted plots turned out.\n\nWe can see that the lable `1` has further been divided into the categories `sexism` and `racism`. Sexism is more prevelant than racism. Racism is looked upon more harshly by the public or twitter might be efficient at removing racist tweets from their site (we never knowü§∑‚Äç‚ôÄÔ∏è).","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(9, 3))\n\nplt.subplot(1, 2, 1)\nsns.countplot(x='num_mentions', data=df, palette =  sns.color_palette(\"mako\", n_colors=9)[3:])\nplt.xlabel('No of mentions')\nplt.ylabel('Count')\nplt.title('Distribution of Mentions (@)')\nadd_spines(linewidth=1)\n\nplt.subplot(1, 2, 2)\nsns.countplot(x='num_hashtags', data=df, palette = sns.color_palette(\"mako\", n_colors=11)[5:])\nplt.xlabel('No of hashtags')\nplt.ylabel('')\nplt.title('Distribution of Hashtags (#)')\nadd_spines(linewidth=1)\n\nplt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.23, hspace=0.3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 3))\nmention_label_cross = pd.crosstab(df['num_mentions'], df['Label'])\nmention_label_cross['sum'] = mention_label_cross[0.0] + mention_label_cross[1.0]\nmention_label_cross['non bullying tweet %'] = mention_label_cross[0.0]/mention_label_cross['sum']*100\nmention_label_cross['bullying tweet %'] = mention_label_cross[1.0]/mention_label_cross['sum']*100\nmention_label_cross = mention_label_cross.drop([0.0, 1.0, 'sum'], axis=1)\n\nmention_label_cross.plot(kind='bar', stacked=True, figsize=(6, 4), color=['#6dd2ac', '#3487a5'], width=0.8)\nplt.xlabel('No of mentions')\nplt.ylabel('percentage %')\nplt.title('Relation between no.of mentions (@) and bullying')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Here you can see that as the mentions increase to 4, the percentage of bullying tweets increases compared to the normal ones. But the trend once again reverses as the mentions increase upto 8. No clear pattern was seen between number of hashtags and percentage of bullying tweets","metadata":{}},{"cell_type":"markdown","source":"### Sentiment analysis\nHere we check whether the sentiment of the tweets directly correlate with bullying. ie. more  negativity a tweet contains, higher the probability of bullying.","metadata":{}},{"cell_type":"code","source":"## Using the nltk library to analyze sentiment of each text so that we can correlate it with bullying\n# Initialize the VADER sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Function to get sentiment of a text\ndef get_sentiment(text):\n    compound_score = sia.polarity_scores(text)['compound']\n    return 'positive' if compound_score >= 0 else 'negative'\n\n\ndf['Sentiment_Label'] = df['Text'].apply(get_sentiment)\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(3, 3))\ndf['Sentiment_Label'].value_counts()\nsns.countplot(x='Sentiment_Label', data=df, palette = bully_palette(df, 'Sentiment_Label', [0]))\nplt.title('Count of sentiment in text')\nplt.xlabel('')\nadd_spines(linewidth=1)\n\nsent_label_cross = pd.crosstab(df['Sentiment_Label'], df['Label'])\nsent_label_cross['sum'] = sent_label_cross[0.0] + sent_label_cross[1.0]\nsent_label_cross['non bullying tweet %'] = sent_label_cross[0.0]/sent_label_cross['sum']*100\nsent_label_cross['bullying tweet %'] = sent_label_cross[1.0]/sent_label_cross['sum']*100\nsent_label_cross = sent_label_cross.drop([0.0, 1.0, 'sum'], axis=1)\n\nax = sent_label_cross.plot(kind='bar', stacked=True, figsize=(6, 4), color=['#6dd2ac', '#3487a5'], width=0.8)\nplt.xlabel('Sentiment')\nplt.ylabel('percentage %')\nplt.title('Relation between sentiment and bullying')\nax.legend(loc='lower right')\nadd_spines(linewidth=1)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" The dataset contains a lot of negative tweets... which is a good thing since our models should be able to understand the difference between a `negative` tweet and a `bullying` tweet. It shouldn't flag every other negative tweet as bullying.","metadata":{}},{"cell_type":"markdown","source":"### Analysis of common hashtags and mentions\n","metadata":{}},{"cell_type":"code","source":"## Extraction of the top ten mentions and hashtags in the dataset\n\n# Function to extract hashtags from a text\ndef extract_hashtags(text):\n    return re.findall(r'#\\w+', text)\n\ndf['Hashtags'] = df['Text'].apply(extract_hashtags)\n\n# Function to extract hashtags from a text\ndef extract_mentions(text):\n    return re.findall(r'@\\w+', text)\n\ndf['Mentions'] = df['Text'].apply(extract_mentions)\n\ndef create_counter_df(df):\n    \"\"\"\n    Creates a dataframe that contains the Label and their count.\n    Pass the dataframe containing the labels in the form of a list column as an input.\n    Example usage:\n    1. mentions = create_counter_df(df['Mentions'])\n    2. sexist_mentions = create_counter_df(df[df.Annotation=='sexist'][\"Mentions\"])\n    \"\"\"\n    # Flatten the list of hashtags and count their occurrences\n    all_counts = [tag for counts_list in df for tag in counts_list]\n    label_counts = Counter(all_counts)\n\n    # Create a DataFrame from the Counter dictionary\n    counts_df = pd.DataFrame(list(label_counts.items()), columns=['Label', 'Count'])\n    counts_df = counts_df.sort_values(by='Count', ascending=False)\n    return counts_df\n    \nmentions_df = create_counter_df(df['Mentions'])\ntop_10_mentions = mentions_df.head(10)\nsexist_mentions = create_counter_df(df[df.Annotation=='sexism'][\"Mentions\"])\ntop_10_sexist_mentions = sexist_mentions.head(10)\nracist_mentions = create_counter_df(df[df.Annotation=='racism'][\"Mentions\"])\ntop_10_racist_mentions = racist_mentions.head(10)\n\nhashtags_df = create_counter_df(df['Hashtags'])\ntop_10_hashtags = hashtags_df.head(10)\nsexist_hashtags = create_counter_df(df[df.Annotation=='sexism'][\"Hashtags\"])\ntop_10_sexist_hashtags = sexist_hashtags.head(10)\nracist_hashtags = create_counter_df(df[df.Annotation=='racism'][\"Hashtags\"])\ntop_10_racist_hashtags = racist_hashtags.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 3))\n\nplt.subplot(121)\nsns.barplot(x='Label', y='Count', data=top_10_hashtags, palette='crest_r')\nplt.title('Top 10 Common Hashtags')\nplt.xlabel('Hashtags')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nadd_spines(linewidth=1)\n\nplt.subplot(122)\nsns.barplot(x='Label', y='Count', data=top_10_mentions, palette='flare_r')\nplt.title('Top 10 Common Mentions')\nplt.xlabel('Mentions')\nplt.ylabel('')\nplt.xticks(rotation=90)\nadd_spines(linewidth=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://raw.githubusercontent.com/harshjadhav890/cyberbullying_detection/main/twitter_%23%40.png\"\n     width=\"300000000\" />\n     \n`#MKR` (@mykitchenrules) is the most popular hashtag in the dataset whereas `@MaxBlumenthal` is the most popular mention.\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\n\nplt.subplot(221)\nsns.barplot(x='Label', y='Count', data=top_10_sexist_hashtags, palette='crest_r')\nplt.title('Top 10 Common Sexist Hashtags')\nplt.xlabel('')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nadd_spines(linewidth=1)\n\nplt.subplot(222)\nsns.barplot(x='Label', y='Count', data=top_10_racist_hashtags, palette='crest_r')\nplt.title('Top 10 Common Racist Hashtags')\nplt.xlabel('')\nplt.ylabel('')\nplt.xticks(rotation=90)\nadd_spines(linewidth=1)\n\nplt.subplot(223)\nsns.barplot(x='Label', y='Count', data=top_10_sexist_mentions, palette='flare_r')\nplt.title('Top 10 Common Sexist Mentions')\nplt.xlabel('Mention')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nadd_spines(linewidth=1)\n\nplt.subplot(224)\nsns.barplot(x='Label', y='Count', data=top_10_racist_mentions, palette='flare_r')\nplt.title('Top 10 Common Racist Mentions')\nplt.xlabel('Mention')\nplt.ylabel('')\nplt.xticks(rotation=90)\nadd_spines(linewidth=1)\n\nplt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.15, hspace=1)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Key takeaways from the most common @ and #s\n\n- For some reason `#MKR` and `#mkr` (my kitchen rules) feature on the top of the sexist hashtags. Maybe coz its a cooking show. lol. It might be due to the fact that #MKR is the most common hashtag on the dataset. <br>\n- You can see religiously motived terrorist groups like `#isis` feature on the most common racist hashtags <br>\n- `@YesYoureSexist` sounds like a provocative username. No wonder it has the second most sexist mentions \n- `@MaxBlumenthal` a controversial American author and blogger features on the top of Racist mentions","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing the text ","metadata":{}},{"cell_type":"code","source":"def clean_tweet(tweet):\n    # Remove URLs\n    tweet = re.sub(r'http\\S+', '', tweet)\n    \n    # Remove mentions and hashtags\n    tweet = re.sub(r'@[A-Za-z0-9_]+|#[A-Za-z0-9_]+', '', tweet)\n    \n    # Remove special characters, numbers, and punctuation\n    tweet = re.sub(r'[^A-Za-z\\s]', '', tweet)\n    \n    # Remove 'RT' (Retweet) indicator\n    tweet = re.sub(r'\\bRT\\b', '', tweet)\n    \n    # Convert to lowercase\n    tweet = tweet.lower()\n    \n    # Remove stopwords\n#     stop_words = set(stopwords.words('english'))\n#     tweet_tokens = nltk.word_tokenize(tweet)\n#     tweet = ' '.join([word for word in tweet_tokens if word not in stop_words])\n    \n    # Lemmatization\n    doc = nlp(tweet)\n    # Lemmatize each token and join them back into a string\n    tweet = ' '.join([token.lemma_ for token in doc])\n    \n    return tweet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Text'] = df['Text'].apply(clean_tweet)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('_________Sample clean tweets_________')\nsimple_tweet = df[df['Label'] == 0]['Text'].iloc[1]\nprint('\\n\\nSimple tweet: ', simple_tweet)\nsexist_tweet = df[df['Annotation'] == 'sexism']['Text'].iloc[0]\nprint('\\n\\nSexist tweet: ', sexist_tweet)\nracist_tweet = df[df['Annotation'] == 'racism']['Text'].iloc[0]\nprint('\\n\\nRacist tweet: ', racist_tweet)\nprint('\\n\\n_______________________________')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ‚òÅÔ∏è Wordclouds for toxic text <a class=\"anchor\"  id=\"section_1_3\"></a>\n","metadata":{}},{"cell_type":"code","source":"stopword=set(STOPWORDS)\nplt.figure(figsize=(15,40))\n\n## Common words in all bullying tweets-----------------------------------------------------------------------------------\nplt.subplot(131)\n# 1. Get text\nbully_text = df[df.Label==1.0][\"Text\"].values\nText = ''\nfor text in bully_text:\n    Text += text\n\n# 2. Load png image from url and create mask\nimage_url = 'https://raw.githubusercontent.com/harshjadhav890/cyberbullying_detection/main/bullying-computer-icons-harassment-clip-art-bully.jpg'\nresponse = requests.get(image_url, stream=True)\nresponse.raise_for_status()\nmask = np.array(Image.open(response.raw))\n\n# 3. Plot wordcloud\nwc = WordCloud(background_color = 'black', mask = mask, contour_width = 2,\n     contour_color = 'black', colormap = 'BuPu_r', width = 800, height = 800, stopwords = stopword).generate(Text)\nplt.axis(\"off\")\nplt.title('Bullying tweets')\nplt.imshow(wc.recolor(colormap= 'summer' , random_state=244), alpha=0.98)\n\n\n## Common words in all racist tweets-----------------------------------------------------------------------------------\n# Repeat for other plots\nplt.subplot(132)\nbully_text = df[df.Annotation=='racism'][\"Text\"].values\nText = ''\nfor text in bully_text:\n    Text += text\n    \nimage_url = 'https://clipart-library.com/img1/1475559.png'\nresponse = requests.get(image_url, stream=True)\nresponse.raise_for_status()\nmask = np.array(Image.open(response.raw))\n\nwc = WordCloud(background_color = 'black', mask = mask, contour_width = 2,\n     contour_color = 'black', colormap = 'BuPu_r', width = 800, height = 800, stopwords = stopword).generate(Text)\nplt.title('Racist tweets')\nplt.axis(\"off\")\nplt.imshow(wc.recolor(colormap= 'Reds' , random_state=244), alpha=0.98)\n\n\n## Common words in all sexist tweets-----------------------------------------------------------------------------------\nplt.subplot(133)\nbully_text = df[df.Annotation=='sexism'][\"Text\"].values\nText = ''\nfor text in bully_text:\n    Text += text\n    \nimage_url = 'https://raw.githubusercontent.com/harshjadhav890/cyberbullying_detection/main/woman-gender-symbol-male-female-text-line-circle-number-png-clipart.jpg'\nresponse.raise_for_status()\nresponse = requests.get(image_url, stream=True)\nmask = np.array(Image.open(response.raw))\n\nwc = WordCloud(background_color = 'black', mask = mask, contour_width = 2,\n     contour_color = 'black', colormap = 'BuPu_r', width = 800, height = 800, stopwords = stopword).generate(Text)\nplt.axis(\"off\")\nplt.title('Sexist tweets')\nplt.imshow(wc.recolor(colormap= 'Paired_r' , random_state=244), alpha=0.98)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üëæ CREATING Models ","metadata":{}},{"cell_type":"code","source":"# ignore this. I did this for ease of processing when i restart the kernel\n# this part gives you the completely preprocessed data right to the point where you just need to oversample and split it\n\ndf = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/twitter_parsed_dataset.csv\")\ndf = df.drop(['id', 'index'], axis=1)\ndf.rename(columns={'oh_label': 'Label'}, inplace=True)\ndf = df.dropna(subset=['Label'])\n\n# Adding hashtags and mentions_____________________________________________________________________________________# \n# here we create a function to count the number of hashtags and mentions and then create two columns to store this data\ndef count_symbols(text):\n#     hashtag_count = len(re.findall(r'#', text))\n    mention_count = len(re.findall(r'@', text))\n#     return hashtag_count, mention_count\n    return mention_count\n\n# Apply the function to each row in the 'text' column\ndf[['num_mentions']] = df['Text'].apply(lambda x: pd.Series(count_symbols(x)))\n# df[['num_hashtags', 'num_mentions']] = df['Text'].apply(lambda x: pd.Series(count_symbols(x)))\n\n\n# Analyze sentiment of each text____________________________________________________________________________________# \nsia = SentimentIntensityAnalyzer()\n\n# Function to get sentiment of a text\ndef get_sentiment(text):\n    compound_score = sia.polarity_scores(text)['compound']\n    return 'positive' if compound_score >= 0 else 'negative'\n\n# Apply the function to each row in the 'Text' column\ndf['Sentiment_Label'] = df['Text'].apply(get_sentiment)\n\n\n# Cleaning the tweets_________________________________________________________________________________________________# \ndef clean_tweet(tweet):\n    # Remove URLs\n    tweet = re.sub(r'http\\S+', '', tweet)\n    \n    # Remove mentions and hashtags\n    tweet = re.sub(r'@[A-Za-z0-9_]+|#[A-Za-z0-9_]+', '', tweet)\n    \n    # Remove special characters, numbers, and punctuation\n    tweet = re.sub(r'[^A-Za-z\\s]', '', tweet)\n    \n    # Remove 'RT' (Retweet) indicator\n    tweet = re.sub(r'\\bRT\\b', '', tweet)\n    \n    # Convert to lowercase\n    tweet = tweet.lower()\n      \n    # Lemmatization\n    doc = nlp(tweet)\n    # Lemmatize each token and join them back into a string\n    tweet = ' '.join([token.lemma_ for token in doc])\n    \n    return tweet\n\ndf['Text'] = df['Text'].apply(clean_tweet)\n#____________________________________________________________________________________________________________________# \n\n# One hot encoding of the sentiment category\none_hot_encoded = pd.get_dummies(df['Sentiment_Label'], prefix='sentiment')\ndf = pd.concat([df, one_hot_encoded], axis=1)\ndf = df.drop('Sentiment_Label', axis=1)\ndf['Text'] = df['Text'].str.replace(r'\\s+', ' ', regex=True).str.strip()\ndf['Text'] = df['Text'].str.strip()\n\n# Step 2: Upsample the data using RandomOverSampler\nros = RandomOverSampler(random_state=42)\nX = df['Text'].values.reshape(-1, 1)\ny = df['Label'].values\nX_resampled, y_resampled = ros.fit_resample(X, y)\n\n# convert text (object) data to string for w2v\nX_resampled= [str(obj) for obj in X_resampled]\nX_resampled = np.array(X_resampled)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 2: Oversample the data using RandomOverSampler to reduce imbalance between Labels (Bullying and non bullying)\nros = RandomOverSampler(random_state=42)\nX = df['Text'].values.reshape(-1, 1)\ny = df['Label'].values\nX_resampled, y_resampled = ros.fit_resample(X, y)\n\n# convert text (object) data to string for w2v\nX_resampled= [str(obj) for obj in X_resampled]\nX_resampled = np.array(X_resampled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ML approach with word2vec <a class=\"anchor\"  id=\"section_2_1\"></a>\n#### Why use Word2Vec?\n- Semantic understanding: Word2Vec captures the semantic relationships between words, which is crucial for understanding the nuances of language used in bullying. It can differentiate between words with similar spellings but different meanings (e.g., \"joke\" vs. \"mock\") and identify synonyms and antonyms, helping to detect subtle insults and sarcasm often employed in cyberbullying.\n\n- Handling slang and abbreviations: Twitter language is full of slang, abbreviations, and emojis. Word2Vec can learn these informal and often-evolving expressions, leading to more accurate detection of bullying even when it doesn't use traditional language patterns.\n\n\n#### Limitations of using Word2Vec\n- Bias: Word2Vec models can inherit biases from the training data, potentially leading to discriminatory outcomes. Careful selection of training data and evaluation methods are crucial.\n- Context sensitivity: Subtle forms of bullying often rely on context not captured solely by word meaning. Combining Word2Vec with other approaches that consider context can improve accuracy.\n\nFor detailed explaination of the intuition behind W2V read: [Word2Vec Explained](https://archive.is/4lJS5)","metadata":{}},{"cell_type":"code","source":"# Train Word2Vec Model\nsentences = [word_tokenize(text) for text in X_resampled]\nword2vec_model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)  # Adjust parameters as needed\n\n# Convert Text to Embeddings\ndef get_embedding(text):\n    tokens = word_tokenize(text)\n    # Filter out tokens that are not in the vocabulary\n    tokens = [token for token in tokens if token in word2vec_model.wv.key_to_index]\n    if len(tokens) > 0:\n        # Return the average of word embeddings for the tokens\n        return np.mean([word2vec_model.wv[t] for t in tokens], axis=0)\n    else:\n        return None\n\n# Create an array of embeddings for each text\nX_resampled = [get_embedding(text) for text in X_resampled]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### üëª Train test split 80:20","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\n# Step 5: Model Training (Using a RandomForestClassifier as an example)\nX_train = np.array(X_train)\nX_test = np.array(X_test)\n\n\"\"\"\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n\nf1 = f1_score(y_test, y_pred)\nprint(f\"F1 Score: {f1}\")\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score\nimport pandas as pd\n\ndef compare_models(x_train, y_train, x_test, y_test):\n  \"\"\"\n  This function compares three classification models: Random Forest, SVM, and Naive Bayes.\n\n  Args:\n      x_train: Training data features.\n      y_train: Training data labels.\n      x_test: Testing data features.\n      y_test: Testing data labels.\n\n  Returns:\n      A pandas DataFrame comparing the models on accuracy, F1-score, and recall.\n  \"\"\"\n\n  # Define models\n  models = {\n      'Random Forest': RandomForestClassifier(),\n      'SVM': SVC(),\n      'Naive Bayes': GaussianNB()\n  }\n\n  # Define evaluation metrics function\n  def evaluate_model(model_name, model):\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    return pd.Series({'Model': model_name, 'Accuracy': accuracy, 'F1-Score': f1, 'Recall': recall})\n\n  # Evaluate models and store results in a DataFrame\n  results = []\n  for name, model in tqdm(models.items(), desc=\"Evaluating Models\"):\n    results.append(evaluate_model(name, model))\n\n  return pd.DataFrame(results).set_index('Model')\n\n# first Spli\nresults_df = compare_models(X_train, y_train, X_test, y_test)\nresults_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train test split 70:30","metadata":{}},{"cell_type":"code","source":"# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n\n# Step 5: Model Training (Using a RandomForestClassifier as an example)\nX_train = np.array(X_train)\nX_test = np.array(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\nresults2_df = compare_models(X_train, y_train, X_test, y_test)\nresults2_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## üß† Deep learning models <a class=\"anchor\"  id=\"section_2_2\"></a>","metadata":{}},{"cell_type":"code","source":"# using the cleaned version of the dataset\nros = RandomOverSampler(random_state=42)\nX = df['Text'].values.reshape(-1, 1)\ny = df['Label'].values\nX_resampled, y_resampled = ros.fit_resample(X, y)\nX_resampled = X_resampled[:, 0]\n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using keras tokenizer to find the length vocab \n#import tensorflow_text\nfrom keras.preprocessing import sequence, text\n#token = tensorflow_text.Tokenizer(num_words=None)\ntoken = text.Tokenizer(num_words=None)\nmax_len = 40\n\ntoken.fit_on_texts(list(X_train) + list(X_test))\nxtrain_seq = token.texts_to_sequences(X_train)\nxvalid_seq = token.texts_to_sequences(X_test)\n\n#zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Preprocessed text', X_train[1])\nprint('\\nTokenized text', xtrain_seq[1])\nprint('\\nPadded text', xtrain_pad[1])\nprint('\\nPadded text Length: ', len(xtrain_pad[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('/kaggle/input/glove840b300dtxt/glove.840B.300d.txt','r',encoding='utf-8')\nfor line in tqdm(f):\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(val) for val in values[1:]])\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:54:11.00454Z","iopub.execute_input":"2024-04-20T14:54:11.004928Z","iopub.status.idle":"2024-04-20T14:57:55.883715Z","shell.execute_reply.started":"2024-04-20T14:54:11.0049Z","shell.execute_reply":"2024-04-20T14:57:55.882743Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:58:08.440855Z","iopub.execute_input":"2024-04-20T14:58:08.44126Z","iopub.status.idle":"2024-04-20T14:58:08.504796Z","shell.execute_reply.started":"2024-04-20T14:58:08.441228Z","shell.execute_reply":"2024-04-20T14:58:08.503897Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **CNN with trainable embedding**","metadata":{}},{"cell_type":"markdown","source":"### 1 -- ration 80:20","metadata":{}},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:46:24.009936Z","iopub.execute_input":"2024-04-21T23:46:24.01033Z","iopub.status.idle":"2024-04-21T23:46:24.016983Z","shell.execute_reply.started":"2024-04-21T23:46:24.010299Z","shell.execute_reply":"2024-04-21T23:46:24.016011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.utils import to_categorical \ny_train_cat = keras.utils.to_categorical(y_train, num_classes=2) ","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:46:28.969234Z","iopub.execute_input":"2024-04-21T23:46:28.969828Z","iopub.status.idle":"2024-04-21T23:46:28.975055Z","shell.execute_reply.started":"2024-04-21T23:46:28.969793Z","shell.execute_reply":"2024-04-21T23:46:28.97409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_cat","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:46:31.961072Z","iopub.execute_input":"2024-04-21T23:46:31.961465Z","iopub.status.idle":"2024-04-21T23:46:31.968421Z","shell.execute_reply.started":"2024-04-21T23:46:31.961435Z","shell.execute_reply":"2024-04-21T23:46:31.967462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#-first/original\n\"\"\"\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Embedding, Dense\nfrom tensorflow.keras.models import Sequential\n\nwith strategy.scope():\n  # Define the CNN model\n  model = Sequential()\n\n  # Embedding layer for word representation\n  model.add(Embedding(len(word_index) + 1,\n                      300,\n                      input_length=max_len))\n\n  # Convolutional layers to extract features\n  model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n  model.add(MaxPooling1D(pool_size=2))\n\n  # Optional additional convolutional layers for more complex features\n  # model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n  # model.add(MaxPooling1D(pool_size=2))\n\n  # Global Average Pooling to capture overall feature representation\n  model.add(GlobalAveragePooling1D())\n\n  # Dense layer for classification\n  model.add(Dense(2, activation='softmax'))\n\n  # Compile the model\n  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:38:33.439138Z","iopub.execute_input":"2024-04-21T22:38:33.43988Z","iopub.status.idle":"2024-04-21T22:38:34.167678Z","shell.execute_reply.started":"2024-04-21T22:38:33.439847Z","shell.execute_reply":"2024-04-21T22:38:34.166686Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with softmax \n\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Embedding, Dense\nfrom tensorflow.keras.models import Sequential\n\nwith strategy.scope():\n  # Define the CNN model\n  model = Sequential()\n\n  # Embedding layer for word representation\n  model.add(Embedding(len(word_index) + 1,\n                      300,\n                      input_length=max_len))\n\n  # Convolutional layers to extract features\n  model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n  model.add(MaxPooling1D(pool_size=2))\n\n  # Optional additional convolutional layers for more complex features\n  # model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n  # model.add(MaxPooling1D(pool_size=2))\n\n  # Global Average Pooling to capture overall feature representation\n  model.add(GlobalAveragePooling1D())\n\n  # Dense layer for multi-class classification\n  model.add(Dense(2, activation='softmax'))  # Update with number of classes\n\n  # Compile the model\n  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:46:43.88006Z","iopub.execute_input":"2024-04-21T23:46:43.880943Z","iopub.status.idle":"2024-04-21T23:46:43.959334Z","shell.execute_reply.started":"2024-04-21T23:46:43.88091Z","shell.execute_reply":"2024-04-21T23:46:43.958467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(xtrain_pad, y_train_cat, epochs=5, batch_size=64*strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:46:50.874985Z","iopub.execute_input":"2024-04-21T23:46:50.875355Z","iopub.status.idle":"2024-04-21T23:47:32.529888Z","shell.execute_reply.started":"2024-04-21T23:46:50.875324Z","shell.execute_reply":"2024-04-21T23:47:32.52887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, recall_score\nimport pandas as pd\n\ndef evaluate_CNNmodel(X_train, X_test, y_train, y_test, model=None):\n  \"\"\"\n  Fits a model, predicts on test data, and returns evaluation metrics.\n\n  Args:\n      X_train: Training data features.\n      X_test: Testing data features.\n      y_train: Training data labels.\n      y_test: Testing data labels.\n      model: The machine learning model to fit and evaluate.\n\n  Returns:\n      A pandas DataFrame containing evaluation metrics (accuracy, F1-score, recall).\n  \"\"\"\n\n  # Fit the model\n  #model.fit(X_train, y_train)\n\n  # Make predictions on test data\n  y_pred = model.predict(X_test)\n  y_pred = np.argmax(y_pred, axis=1)\n\n  # Calculate evaluation metrics\n  accuracy = accuracy_score(y_test, y_pred)\n  f1 = f1_score(y_test, y_pred)\n  recall = recall_score(y_test, y_pred)\n\n  # Create a DataFrame with the results\n  results_df = pd.DataFrame({\n      \"Metric\": [\"Accuracy\", \"F1-Score\", \"Recall\"],\n      \"Value\": [accuracy, f1, recall]\n  })\n\n  return results_df, \n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:47:32.531451Z","iopub.execute_input":"2024-04-21T23:47:32.531727Z","iopub.status.idle":"2024-04-21T23:47:32.539671Z","shell.execute_reply.started":"2024-04-21T23:47:32.531703Z","shell.execute_reply":"2024-04-21T23:47:32.538681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = evaluate_CNNmodel(X_train, xvalid_pad, y_train, y_test, model)\nprint(model_results)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:47:32.540763Z","iopub.execute_input":"2024-04-21T23:47:32.541035Z","iopub.status.idle":"2024-04-21T23:47:32.955067Z","shell.execute_reply.started":"2024-04-21T23:47:32.541011Z","shell.execute_reply":"2024-04-21T23:47:32.954109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2 -- Ration 70:30","metadata":{}},{"cell_type":"code","source":"# using the cleaned version of the dataset\nros = RandomOverSampler(random_state=42)\nX = df['Text'].values.reshape(-1, 1)\ny = df['Label'].values\nX_resampled, y_resampled = ros.fit_resample(X, y)\nX_resampled = X_resampled[:, 0]\n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:49:39.289516Z","iopub.execute_input":"2024-04-21T23:49:39.290324Z","iopub.status.idle":"2024-04-21T23:49:39.304804Z","shell.execute_reply.started":"2024-04-21T23:49:39.29029Z","shell.execute_reply":"2024-04-21T23:49:39.303709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# using keras tokenizer to find the length vocab \n#import tensorflow_text\nfrom keras.preprocessing import sequence, text\n#token = tensorflow_text.Tokenizer(num_words=None)\ntoken = text.Tokenizer(num_words=None)\nmax_len = 40\n\ntoken.fit_on_texts(list(X_train) + list(X_test))\nxtrain_seq = token.texts_to_sequences(X_train)\nxvalid_seq = token.texts_to_sequences(X_test)\n\n#zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:50:22.610611Z","iopub.execute_input":"2024-04-21T23:50:22.611479Z","iopub.status.idle":"2024-04-21T23:50:23.541512Z","shell.execute_reply.started":"2024-04-21T23:50:22.611437Z","shell.execute_reply":"2024-04-21T23:50:23.540701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_cat = keras.utils.to_categorical(y_train, num_classes=2) \n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:50:28.600942Z","iopub.execute_input":"2024-04-21T23:50:28.601719Z","iopub.status.idle":"2024-04-21T23:50:28.606504Z","shell.execute_reply.started":"2024-04-21T23:50:28.601684Z","shell.execute_reply":"2024-04-21T23:50:28.605566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with softmax \n\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Embedding, Dense\nfrom tensorflow.keras.models import Sequential\n\nwith strategy.scope():\n  # Define the CNN model\n  model = Sequential()\n\n  # Embedding layer for word representation\n  model.add(Embedding(len(word_index) + 1,\n                      300,\n                      input_length=max_len))\n\n  # Convolutional layers to extract features\n  model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n  model.add(MaxPooling1D(pool_size=2))\n\n  # Optional additional convolutional layers for more complex features\n  # model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n  # model.add(MaxPooling1D(pool_size=2))\n\n  # Global Average Pooling to capture overall feature representation\n  model.add(GlobalAveragePooling1D())\n\n  # Dense layer for multi-class classification\n  model.add(Dense(2, activation='softmax'))  # Update with number of classes\n\n  # Compile the model\n  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:50:34.243354Z","iopub.execute_input":"2024-04-21T23:50:34.244218Z","iopub.status.idle":"2024-04-21T23:50:34.324678Z","shell.execute_reply.started":"2024-04-21T23:50:34.244183Z","shell.execute_reply":"2024-04-21T23:50:34.3237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(xtrain_pad, y_train_cat, epochs=5, batch_size=64*strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:50:42.348988Z","iopub.execute_input":"2024-04-21T23:50:42.349351Z","iopub.status.idle":"2024-04-21T23:51:07.988668Z","shell.execute_reply.started":"2024-04-21T23:50:42.349322Z","shell.execute_reply":"2024-04-21T23:51:07.987733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = evaluate_CNNmodel(X_train, xvalid_pad, y_train, y_test, model)\nprint(model_results)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:51:07.990168Z","iopub.execute_input":"2024-04-21T23:51:07.990495Z","iopub.status.idle":"2024-04-21T23:51:08.568844Z","shell.execute_reply.started":"2024-04-21T23:51:07.990466Z","shell.execute_reply":"2024-04-21T23:51:08.567712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚¨ÖÔ∏è‚û°Ô∏è DISTILLBERT with Tensorflow Hub <a class=\"anchor\"  id=\"section_2_3\"></a>","metadata":{}},{"cell_type":"code","source":"ros = RandomOverSampler(random_state=42)\nX = df['Text'].values.reshape(-1, 1)\ny = df['Label'].values\nX_resampled, y_resampled = ros.fit_resample(X, y)\nX_resampled = X_resampled[:, 0]\n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:31:38.613965Z","iopub.execute_input":"2024-04-22T05:31:38.615085Z","iopub.status.idle":"2024-04-22T05:31:38.630297Z","shell.execute_reply.started":"2024-04-22T05:31:38.615024Z","shell.execute_reply":"2024-04-22T05:31:38.629403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.utils import to_categorical \ny_train_cat = keras.utils.to_categorical(y_train, num_classes=2)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:31:45.824202Z","iopub.execute_input":"2024-04-22T05:31:45.825113Z","iopub.status.idle":"2024-04-22T05:31:45.830482Z","shell.execute_reply.started":"2024-04-22T05:31:45.82507Z","shell.execute_reply":"2024-04-22T05:31:45.829543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating tensorflow datasets \ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train_cat))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test))\n\nBUFFER_SIZE = 4000\nBATCH_SIZE = 64\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:31:49.047484Z","iopub.execute_input":"2024-04-22T05:31:49.048431Z","iopub.status.idle":"2024-04-22T05:31:49.067114Z","shell.execute_reply.started":"2024-04-22T05:31:49.048389Z","shell.execute_reply":"2024-04-22T05:31:49.065997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train dataset element spec:\", train_dataset.element_spec)\nprint(\"Test dataset element spec:\", test_dataset.element_spec)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:31:52.895109Z","iopub.execute_input":"2024-04-22T05:31:52.895586Z","iopub.status.idle":"2024-04-22T05:31:52.900819Z","shell.execute_reply.started":"2024-04-22T05:31:52.895554Z","shell.execute_reply":"2024-04-22T05:31:52.899885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for example, label in train_dataset.take(1): # take returns a batch of train_dataset, containing 64 feature, label pairs.\n    print('texts: ', example.numpy()[:3])\n    print()\n    print('labels: ', label.numpy()[:3])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:31:58.632951Z","iopub.execute_input":"2024-04-22T05:31:58.633316Z","iopub.status.idle":"2024-04-22T05:31:58.656971Z","shell.execute_reply.started":"2024-04-22T05:31:58.633286Z","shell.execute_reply":"2024-04-22T05:31:58.65602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the preprocessing and main model from tf hub links\n\n#tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/2'\n#tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n\ntfhub_handle_encoder = 'https://www.kaggle.com/models/jeongukjae/distilbert/TensorFlow2/en-cased-l-6-h-768-a-12/1'\ntfhub_handle_preprocess = 'https://kaggle.com/models/jeongukjae/distilbert/TensorFlow2/en-cased-preprocess/2'\n\nbert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\nbert_model = hub.KerasLayer(tfhub_handle_encoder)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:32:04.037092Z","iopub.execute_input":"2024-04-22T05:32:04.037482Z","iopub.status.idle":"2024-04-22T05:32:18.176808Z","shell.execute_reply.started":"2024-04-22T05:32:04.037451Z","shell.execute_reply":"2024-04-22T05:32:18.176018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we take some sample input texts and test the bert preprocess model on them. We analyze the outputs given by the model below","metadata":{}},{"cell_type":"code","source":"# Testing the bert preprocessor\n\ntext_test = [\"Today's weather is harsh\", \"My name is harsh\"]\ntext_preprocessed = bert_preprocess_model(text_test)\n\nfor i in range(len(text_test)):\n    print('Input text :', text_test[i])\n    print(f'Keys       : {list(text_preprocessed.keys())}')\n    print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n    print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][i, :12]}')\n    print(f'Input Mask : {text_preprocessed[\"input_mask\"][i, :12]}')\n    #print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][i, :12]}')\n    print('\\n')\n# input_type_ids has the same shape as input_mask, but inside the non-padded region,\n# contains a 0 or a 1 indicating which sentence the token is a part of.\n\n# context is not being mapped at the preprocessing layer\n# The word 'Harsh' will have the same token no matter what the scenario\n# BERT will perform the contextualization when the text is passed through it","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:32:18.182378Z","iopub.execute_input":"2024-04-22T05:32:18.182647Z","iopub.status.idle":"2024-04-22T05:32:19.481525Z","shell.execute_reply.started":"2024-04-22T05:32:18.182624Z","shell.execute_reply":"2024-04-22T05:32:19.480479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model link: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n\nbert_results = bert_model(text_preprocessed)\n\nprint(f'Loaded BERT: {tfhub_handle_encoder}')\nprint(f'Keys       : {list(bert_results.keys())}')\nprint(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\nprint(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\nprint(f'Encoder Outputs Shape:{bert_results[\"encoder_outputs\"][0].shape}')\n# print(f'Encoder Outputs Values:{bert_results[\"encoder_outputs\"][0][0, :3]}')\nprint(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n# print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :3]}')\n\n# BERT takes our sentence and outputs a 768 token pooled representation, where every word is mapped in a 128 dimension array","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:32:19.482737Z","iopub.execute_input":"2024-04-22T05:32:19.483035Z","iopub.status.idle":"2024-04-22T05:32:19.842112Z","shell.execute_reply.started":"2024-04-22T05:32:19.483011Z","shell.execute_reply":"2024-04-22T05:32:19.841139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The pooled outputs provide a fixed-size representation of the entire input sequence,\n- The encoder outputs represent the contextualized embeddings of each token, and the sequence\n- Outputs retain token-level information for more detailed analysis or tasks.\n- Either of those can be used as input to further model building depending on granularity of the task","metadata":{}},{"cell_type":"code","source":"# first/original\n\n# A function that takes in the links for the model and returns a newly compiled model\n\ndef create_model(model_link, preprocess_link):\n  # bert_model = hub.KerasLayer(model_link)\n  # bert_preprocess_model = hub.KerasLayer(preprcess_link)\n\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(preprocess_link, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(model_link, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n\n    model = tf.keras.Model(text_input, net)\n\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    metrics = tf.metrics.BinaryAccuracy()\n    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n                              loss=loss,\n                              metrics=metrics)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:51:36.069779Z","iopub.execute_input":"2024-04-21T22:51:36.070084Z","iopub.status.idle":"2024-04-21T22:51:36.078027Z","shell.execute_reply.started":"2024-04-21T22:51:36.070057Z","shell.execute_reply":"2024-04-21T22:51:36.077104Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A function that takes in the links for the model and returns a newly compiled model\n\ndef create_model(model_link, preprocess_link):\n  # bert_model = hub.KerasLayer(model_link)\n  # bert_preprocess_model = hub.KerasLayer(preprcess_link)\n\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(preprocess_link, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(model_link, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(2, activation='softmax', name='classifier')(net)\n\n    model = tf.keras.Model(text_input, net)\n\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    metrics = tf.metrics.BinaryAccuracy()\n    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n                              loss=loss,\n                              metrics=metrics)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:32:23.124394Z","iopub.execute_input":"2024-04-22T05:32:23.124751Z","iopub.status.idle":"2024-04-22T05:32:23.132781Z","shell.execute_reply.started":"2024-04-22T05:32:23.124725Z","shell.execute_reply":"2024-04-22T05:32:23.131847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, recall_score\nimport pandas as pd\n\ndef evaluate_DBmodel(X_test, y_test, model=None):\n  \"\"\"\n  Fits a model, predicts on test data, and returns evaluation metrics.\n\n  Args:\n      X_train: Training data features.\n      X_test: Testing data features.\n      y_train: Training data labels.\n      y_test: Testing data labels.\n      model: The machine learning model to fit and evaluate.\n\n  Returns:\n      A pandas DataFrame containing evaluation metrics (accuracy, F1-score, recall).\n  \"\"\"\n\n  # Fit the model\n  #model.fit(X_train, y_train)\n\n  # Make predictions on test data\n  y_pred = tf.sigmoid(model(tf.constant(X_test)))\n  #y_pred = tf.nn.softmax(model(tf.constant(X_test)))\n  y_pred = np.argmax(y_pred, axis=1)\n\n  # Calculate evaluation metrics\n  accuracy = accuracy_score(y_test, y_pred)\n  f1 = f1_score(y_test, y_pred)\n  recall = recall_score(y_test, y_pred)\n\n  # Create a DataFrame with the results\n  results_df = pd.DataFrame({\n      \"Metric\": [\"Accuracy\", \"F1-Score\", \"Recall\"],\n      \"Value\": [accuracy, f1, recall]\n  })\n\n  return results_df, \n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:32:42.638059Z","iopub.execute_input":"2024-04-22T05:32:42.638471Z","iopub.status.idle":"2024-04-22T05:32:42.646495Z","shell.execute_reply.started":"2024-04-22T05:32:42.638438Z","shell.execute_reply":"2024-04-22T05:32:42.645528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-22T00:46:58.562125Z","iopub.execute_input":"2024-04-22T00:46:58.562583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training DistilBert on ratio 80:20","metadata":{}},{"cell_type":"code","source":"DATA_SIZE = tf.data.experimental.cardinality(train_dataset).numpy()\n\nprint(\"Size of the dataset:\", DATA_SIZE)\nBATCH_SIZE = 16\nSTEPS_PER_EPOCH = DATA_SIZE // BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:32:49.822001Z","iopub.execute_input":"2024-04-22T05:32:49.822345Z","iopub.status.idle":"2024-04-22T05:32:49.828453Z","shell.execute_reply.started":"2024-04-22T05:32:49.822318Z","shell.execute_reply":"2024-04-22T05:32:49.827378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bert_en_uncased_L-4_H-256_A-4/2\n\nwith strategy.scope():\n    model = create_model(tfhub_handle_encoder, tfhub_handle_preprocess)\nmodel.fit(train_dataset, epochs=5, batch_size=16)\n# directly passed in the train_dataset tensor to the model as the preprocessing layer takes care of embedding","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:37:37.775222Z","iopub.execute_input":"2024-04-22T05:37:37.775614Z","iopub.status.idle":"2024-04-22T05:52:31.805302Z","shell.execute_reply.started":"2024-04-22T05:37:37.775579Z","shell.execute_reply":"2024-04-22T05:52:31.804173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since the model is showing continued improvements let us train it some more\n#model.fit(train_dataset, epochs=5, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:26:00.274916Z","iopub.execute_input":"2024-01-14T18:26:00.275301Z","iopub.status.idle":"2024-01-14T18:31:15.878951Z","shell.execute_reply.started":"2024-01-14T18:26:00.275266Z","shell.execute_reply":"2024-01-14T18:31:15.878058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = evaluate_DBmodel(X_test[:2000], y_test[:2000], model)\nprint(model_results)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:05:20.427138Z","iopub.execute_input":"2024-04-22T06:05:20.427517Z","iopub.status.idle":"2024-04-22T06:07:24.098476Z","shell.execute_reply.started":"2024-04-22T06:05:20.427486Z","shell.execute_reply":"2024-04-22T06:07:24.09751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_pred = tf.sigmoid(model(tf.constant(X_test)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train DistilBert on ratio 70:30","metadata":{}},{"cell_type":"code","source":"ros = RandomOverSampler(random_state=42)\nX = df['Text'].values.reshape(-1, 1)\ny = df['Label'].values\nX_resampled, y_resampled = ros.fit_resample(X, y)\nX_resampled = X_resampled[:, 0]\n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n\ny_train_cat = keras.utils.to_categorical(y_train, num_classes=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:08:28.106323Z","iopub.execute_input":"2024-04-22T06:08:28.107256Z","iopub.status.idle":"2024-04-22T06:08:28.122194Z","shell.execute_reply.started":"2024-04-22T06:08:28.107218Z","shell.execute_reply":"2024-04-22T06:08:28.121159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating tensorflow datasets \ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train_cat))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test))\n\nBUFFER_SIZE = 4000\nBATCH_SIZE = 64\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:08:31.185811Z","iopub.execute_input":"2024-04-22T06:08:31.186979Z","iopub.status.idle":"2024-04-22T06:08:31.20495Z","shell.execute_reply.started":"2024-04-22T06:08:31.186931Z","shell.execute_reply":"2024-04-22T06:08:31.204075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_SIZE = tf.data.experimental.cardinality(train_dataset).numpy()\n\nprint(\"Size of the dataset:\", DATA_SIZE)\nBATCH_SIZE = 16\nSTEPS_PER_EPOCH = DATA_SIZE // BATCH_SIZE\n\n\n\n# bert_en_uncased_L-4_H-256_A-4/2\n\nwith strategy.scope():\n    model = create_model(tfhub_handle_encoder, tfhub_handle_preprocess)\nmodel.fit(train_dataset, epochs=5, batch_size=16)\n# directly passed in the train_dataset tensor to the model as the preprocessing layer takes care of embedding\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:12:27.680309Z","iopub.execute_input":"2024-04-22T06:12:27.681018Z","iopub.status.idle":"2024-04-22T06:25:31.628794Z","shell.execute_reply.started":"2024-04-22T06:12:27.680982Z","shell.execute_reply":"2024-04-22T06:25:31.627865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = evaluate_DBmodel(test_dataset, y_test, model)\nprint(model_results)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:28:43.832302Z","iopub.execute_input":"2024-04-22T06:28:43.833209Z","iopub.status.idle":"2024-04-22T06:28:43.946391Z","shell.execute_reply.started":"2024-04-22T06:28:43.833172Z","shell.execute_reply":"2024-04-22T06:28:43.945247Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = tf.sigmoid(model(tf.constant(X_test[:1000])))\ny_pred = np.argmax(y_pred, axis=1)\n# Calculate evaluation metrics\naccuracy = accuracy_score(y_test[:1000], y_pred)\nf1 = f1_score(y_test[:1000], y_pred)\nrecall = recall_score(y_test[:1000], y_pred)\nprint(recall)\n\n# Create a DataFrame with the results\nresults_df = pd.DataFrame({\n      \"Metric\": [\"Accuracy\", \"F1-Score\", \"Recall\"],\n      \"Value\": [accuracy, f1, recall]\n  })\nresults_df","metadata":{"execution":{"iopub.status.busy":"2024-04-22T07:07:14.90499Z","iopub.execute_input":"2024-04-22T07:07:14.905725Z","iopub.status.idle":"2024-04-22T07:07:47.444284Z","shell.execute_reply.started":"2024-04-22T07:07:14.90569Z","shell.execute_reply":"2024-04-22T07:07:47.443301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = tf.sigmoid(model(tf.constant(X_test)))\ny_pred = np.argmax(y_pred, axis=1)\n# Calculate evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n#print(recall)\n\n# Create a DataFrame with the results\nresults_df = pd.DataFrame({\n      \"Metric\": [\"Accuracy\", \"F1-Score\", \"Recall\"],\n      \"Value\": [accuracy, f1, recall]\n  })\nresults_df","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:37:48.905983Z","iopub.execute_input":"2024-04-22T06:37:48.906855Z"},"trusted":true},"execution_count":null,"outputs":[]}]}